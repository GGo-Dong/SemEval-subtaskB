import pandas as pd
import xgboost as xgb
from sklearn.metrics import f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import os

# ================= ì„¤ì • =================
FEATURE_DIR = "features"
DATA_DIR = "data" # ì›ë³¸ ë¼ë²¨ í™•ì¸ìš©

# 1. Feature íŒŒì¼ë“¤ ë¡œë“œ ë° ë³‘í•©
def load_features(split_name):
    # UnixCoder Features
    unix_df = pd.read_parquet(os.path.join(FEATURE_DIR, f"unix_{split_name}.parquet"))
    # TF-IDF Features
    tfidf_df = pd.read_parquet(os.path.join(FEATURE_DIR, f"tfidf_{split_name}.parquet"))
    
    # ì˜†ìœ¼ë¡œ í•©ì¹˜ê¸° (Concat)
    # unix_dfì—ëŠ” ì´ë¯¸ 'label'ì´ í¬í•¨ë˜ì–´ ìˆìŒ
    merged_df = pd.concat([unix_df, tfidf_df], axis=1)
    return merged_df

print("ğŸ”„ Merging Features...")
train_df = load_features("train")
val_df = load_features("val")

# í•™ìŠµì— ì‚¬ìš©í•  Feature ì»¬ëŸ¼ ì •ì˜ (label ì œì™¸)
feature_cols = [c for c in train_df.columns if c != 'label']
print(f"ğŸ“Œ Using {len(feature_cols)} features.")

# 2. XGBoost í•™ìŠµ
print("ğŸš€ Training XGBoost Meta-Learner...")

# ë¼ë²¨ì´ ë¬¸ìì—´ì´ë¼ë©´ ìˆ«ìë¡œ ë³€í™˜ í•„ìš” (ì´ë¯¸ ìˆ«ìì¸ ê²½ìš° ìƒëµ ê°€ëŠ¥)
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(train_df['label'])
y_val = le.transform(val_df['label'])

dtrain = xgb.DMatrix(train_df[feature_cols], label=y_train)
dval = xgb.DMatrix(val_df[feature_cols], label=y_val)

params = {
    'objective': 'multi:softmax',
    'num_class': len(le.classes_),
    'max_depth': 6,
    'eta': 0.1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'eval_metric': 'mlogloss'
}

model = xgb.train(
    params, 
    dtrain, 
    num_boost_round=500, 
    evals=[(dtrain, 'train'), (dval, 'val')],
    early_stopping_rounds=50,
    verbose_eval=50
)

print("\nğŸ“Š Evaluating...")
preds = model.predict(dval)
f1 = f1_score(y_val, preds, average='macro')
print(f"ğŸ† Final Macro F1 Score: {f1:.4f}")

# [ìˆ˜ì •] target_namesë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì—ëŸ¬ í•´ê²° í•µì‹¬!)
target_names_str = le.classes_.astype(str)

print(classification_report(y_val, preds, target_names=target_names_str))

# Feature Importance ì‹œê°í™” (ì´ê²Œ ì§„ì§œ ì¤‘ìš”í•©ë‹ˆë‹¤!)
plt.figure(figsize=(10, 8)) # ê·¸ë¦¼ í¬ê¸° í‚¤ì›€
xgb.plot_importance(model, max_num_features=20, height=0.5)
plt.title("Feature Importance (TF-IDF vs UniXcoder)")
plt.tight_layout()
plt.savefig("xgb_feature_importance.png")
print("ğŸ–¼ï¸ Saved feature importance plot: xgb_feature_importance.png")