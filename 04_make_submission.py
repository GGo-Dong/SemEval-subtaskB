import pandas as pd
import xgboost as xgb
import os
from sklearn.preprocessing import LabelEncoder
import numpy as np

# ================= ì„¤ì • =================
FEATURE_DIR = "features"
DATA_DIR = "data"
SUBMISSION_FILE = "submission.csv"

# 1. Feature ë¡œë“œ í•¨ìˆ˜ (UnixCoder + TF-IDF ë³‘í•©)
def load_merged_features(split_name):
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    unix_path = os.path.join(FEATURE_DIR, f"unix_{split_name}.parquet")
    tfidf_path = os.path.join(FEATURE_DIR, f"tfidf_{split_name}.parquet")
    
    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(unix_path):
        raise FileNotFoundError(f"âŒ {unix_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. 01ë²ˆ ì½”ë“œë¥¼ ì‹¤í–‰í–ˆë‚˜ìš”?")
    if not os.path.exists(tfidf_path):
        raise FileNotFoundError(f"âŒ {tfidf_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. 02ë²ˆ ì½”ë“œë¥¼ ì‹¤í–‰í–ˆë‚˜ìš”?")

    print(f"ğŸ”„ Merging features for {split_name}...")
    unix_df = pd.read_parquet(unix_path)
    tfidf_df = pd.read_parquet(tfidf_path)
    
    # ì˜†ìœ¼ë¡œ í•©ì¹˜ê¸° (Concat)
    merged_df = pd.concat([unix_df, tfidf_df], axis=1)
    return merged_df

# ================= ë©”ì¸ ì‹¤í–‰ ë¡œì§ =================

# 1. ë°ì´í„° ë¡œë“œ (Train + Valì„ ëª¨ë‘ í•©ì³ì„œ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©)
print("ğŸ“‚ Loading All Features...")
train_feat = load_merged_features("train")
val_feat = load_merged_features("val")
full_train_df = pd.concat([train_feat, val_feat], axis=0).reset_index(drop=True)

# Test ë°ì´í„° ë¡œë“œ
test_df = load_merged_features("test")

# í•™ìŠµì— ì‚¬ìš©í•  Feature ì»¬ëŸ¼ ì •ì˜ (label ì œì™¸)
feature_cols = [c for c in full_train_df.columns if c != 'label']
print(f"ğŸ“Œ Training on {len(full_train_df)} samples with {len(feature_cols)} features.")

# 2. ë¼ë²¨ ì¸ì½”ë”© (ë¬¸ìì—´ -> ìˆ«ì)
# XGBoostëŠ” ìˆ«ì ë¼ë²¨ë§Œ ë¨¹ìœ¼ë¯€ë¡œ ë³€í™˜ í•„ìš”
le = LabelEncoder()
y_train = le.fit_transform(full_train_df['label'])

# 3. XGBoost ì „ì²´ ë°ì´í„° ì¬í•™ìŠµ
print("ğŸš€ Retraining XGBoost on Full Data (Train + Val)...")
dtrain = xgb.DMatrix(full_train_df[feature_cols], label=y_train)
dtest = xgb.DMatrix(test_df[feature_cols])

params = {
    'objective': 'multi:softmax',
    'num_class': len(le.classes_),
    'max_depth': 6,          # 03ë²ˆì—ì„œ ê²€ì¦ëœ íŒŒë¼ë¯¸í„°
    'eta': 0.1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'eval_metric': 'mlogloss',
    # [ìˆ˜ì •] CPU ìŠ¤ë ˆë“œ ì„¤ì • ì œê±°í•˜ê³  GPU ì„¤ì • ì¶”ê°€
    # 'nthread': -1,  <-- ì œê±° ë˜ëŠ” ì£¼ì„ ì²˜ë¦¬
    'device': 'cuda', # <-- ìµœì‹  XGBoost ë²„ì „ì¼ ê²½ìš°
    'tree_method': 'hist', # <-- GPU ê°€ì† íˆìŠ¤í† ê·¸ë¨ ë°©ì‹
}

# Epoch 500íšŒ (ì•„ê¹Œ ë¡œê·¸ ë³´ë‹ˆ 100~500 ì‚¬ì´ì—ì„œ ì¶©ë¶„íˆ ìˆ˜ë ´í•¨)
model = xgb.train(params, dtrain, num_boost_round=500, verbose_eval=50)

# 4. ì˜ˆì¸¡ ë° ë³µì›
print("ğŸ”® Predicting on Test Set...")
preds = model.predict(dtest)
pred_labels = le.inverse_transform(preds.astype(int)) # ìˆ«ì(0,1,..) -> ì›ë˜ ë¬¸ìì—´(Mistral,..)

# 5. Submission CSV ìƒì„±
print("ğŸ’¾ Creating Submission File...")

# ì›ë³¸ Test íŒŒì¼ì—ì„œ ID ê°€ì ¸ì˜¤ê¸°
origin_test_path = os.path.join(DATA_DIR, "test.parquet")
if not os.path.exists(origin_test_path):
     # í˜¹ì‹œ íŒŒì¼ëª…ì´ ë‹¤ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ sample íŒŒì¼ ì²´í¬
    origin_test_path = os.path.join(DATA_DIR, "task_b_test_set_sample.parquet")

origin_test = pd.read_parquet(origin_test_path)

# ID ì»¬ëŸ¼ í™•ì¸ (ë³´í†µ 'id'ì„)
id_col = 'id' if 'id' in origin_test.columns else origin_test.columns[0]
print(f"   - Using ID column: {id_col}")

submission = pd.DataFrame({
    id_col: origin_test[id_col],
    'label': pred_labels
})

submission.to_csv(SUBMISSION_FILE, index=False)
print(f"âœ… Submission file saved: {SUBMISSION_FILE}")
print(submission.head())