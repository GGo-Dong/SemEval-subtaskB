import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
import os
import numpy as np

# ================= ì„¤ì • =================
MODEL_NAME = "microsoft/graphcodebert-base"
MODEL_PATH = "models/best_graphcodebert.pth"
DATA_DIR = "data"
SAVE_DIR = "features"
BATCH_SIZE = 32  # ì¶”ë¡ (Inference) ë•ŒëŠ” í•™ìŠµë³´ë‹¤ ë©”ëª¨ë¦¬ë¥¼ ëœ ë¨¹ìœ¼ë‹ˆ í‚¤ì›Œë„ ë©ë‹ˆë‹¤
MAX_LEN = 512
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ================= í´ë˜ìŠ¤ ì •ì˜ (í•™ìŠµ ë•Œì™€ ë™ì¼) =================
class GraphCodeBERTClassifier(nn.Module):
    def __init__(self, model_name, num_classes):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(model_name)
        self.classifier = nn.Linear(self.encoder.config.hidden_size, num_classes)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, input_ids, attention_mask):
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0, :]
        logits = self.classifier(pooled_output)
        return logits

class CodeDataset(Dataset):
    def __init__(self, df, tokenizer, max_len=512):
        self.codes = df['code'].fillna("").astype(str).tolist()
        self.tokenizer = tokenizer
        self.max_len = max_len
    
    def __len__(self):
        return len(self.codes)
    
    def __getitem__(self, idx):
        enc = self.tokenizer(self.codes[idx], truncation=True, padding='max_length', 
                             max_length=self.max_len, return_tensors='pt')
        return {
            'input_ids': enc['input_ids'].squeeze(0),
            'attention_mask': enc['attention_mask'].squeeze(0)
        }

# ================= ì¶”ì¶œ í•¨ìˆ˜ =================
def extract_and_save(loader, df, filename, model):
    print(f"ğŸš€ Extracting features for {filename}...")
    model.eval()
    all_probs = []
    
    with torch.no_grad():
        for batch in tqdm(loader):
            input_ids = batch['input_ids'].to(device)
            mask = batch['attention_mask'].to(device)
            
            logits = model(input_ids, mask)
            probs = torch.softmax(logits, dim=1) # í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜
            all_probs.extend(probs.cpu().numpy())
            
    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì €ì¥ (gcb_prob_0, gcb_prob_1 ... í˜•íƒœ)
    prob_cols = [f'gcb_prob_{i}' for i in range(len(all_probs[0]))]
    out_df = pd.DataFrame(all_probs, columns=prob_cols)
    
    # IDê°€ ìˆìœ¼ë©´ ê°™ì´ ì €ì¥ (ë‚˜ì¤‘ì— í•©ì¹  ë•Œ ì•ˆì „ì¥ì¹˜)
    if 'id' in df.columns:
        out_df['id'] = df['id'].values
    
    if not os.path.exists(SAVE_DIR):
        os.makedirs(SAVE_DIR)
        
    save_path = os.path.join(SAVE_DIR, f"gcb_{filename}")
    out_df.to_parquet(save_path)
    print(f"âœ… Saved to {save_path}")

# ================= ë©”ì¸ ì‹¤í–‰ =================
# 1. ëª¨ë¸ ë¡œë“œ
print("ğŸ“‚ Loading Model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = GraphCodeBERTClassifier(MODEL_NAME, num_classes=11).to(device)
model.load_state_dict(torch.load(MODEL_PATH, map_location=device))

# 2. ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
def load_data(fname):
    path = os.path.join(DATA_DIR, fname)
    if not os.path.exists(path):
        # í…ŒìŠ¤íŠ¸ì…‹ ì´ë¦„ì´ ë‹¤ë¥¼ ê²½ìš° ëŒ€ë¹„
        path = os.path.join(DATA_DIR, "task_b_test_set_sample.parquet") if "test" in fname else path
    df = pd.read_parquet(path)
    if 'code' not in df.columns and 'text' in df.columns:
        df['code'] = df['text']
    return df

# Train / Val / Test ë¡œë“œ
train_df = load_data("task_b_training_set.parquet")
val_df = load_data("task_b_validation_set.parquet")
test_df = load_data("test.parquet") # íŒŒì¼ëª… í™•ì¸ í•„ìš”

# ë°ì´í„°ì…‹ & ë¡œë” ìƒì„±
train_loader = DataLoader(CodeDataset(train_df, tokenizer, MAX_LEN), batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
val_loader = DataLoader(CodeDataset(val_df, tokenizer, MAX_LEN), batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
test_loader = DataLoader(CodeDataset(test_df, tokenizer, MAX_LEN), batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

# 3. ì¶”ì¶œ ì‹¤í–‰
extract_and_save(train_loader, train_df, "train.parquet", model)
extract_and_save(val_loader, val_df, "val.parquet", model)
extract_and_save(test_loader, test_df, "test.parquet", model)