import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
import os

# ================= ì„¤ì • =================
MODEL_PATH = "models/best_model_epoch_4.pth"  # ì‚¬ìš©ìê°€ ê°€ì§„ pth íŒŒì¼ ê²½ë¡œ
BASE_MODEL = "microsoft/unixcoder-base"
DATA_DIR = "data"
SAVE_DIR = "features"
BATCH_SIZE = 128
NUM_CLASSES = 11  # ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ê°œìˆ˜

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”¥ Current Device: {device}")
os.makedirs(SAVE_DIR, exist_ok=True)

# ================= ëª¨ë¸ ì •ì˜ (ê¸°ì¡´ê³¼ ë™ì¼í•´ì•¼ í•¨) =================
class UniXcoderClassifier(nn.Module):
    def __init__(self, base_model_name, num_classes):
        super().__init__()
        self.unixcoder = AutoModel.from_pretrained(base_model_name)
        self.classifier = nn.Linear(self.unixcoder.config.hidden_size, num_classes)
        self.dropout = nn.Dropout(0.3) # í•™ìŠµë•Œ ì¼ë˜ ë“œë¡­ì•„ì›ƒ (Inferenceì—” ì˜í–¥ X)
        self.layer_norm = nn.LayerNorm(self.unixcoder.config.hidden_size)

    def forward(self, input_ids, attention_mask):
        outputs = self.unixcoder(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0, :]
        pooled_output = self.layer_norm(pooled_output)
        logits = self.classifier(pooled_output)
        return logits

# ================= ë°ì´í„°ì…‹ ì •ì˜ =================
class SimpleCodeDataset(Dataset):
    def __init__(self, df, tokenizer, max_len=512):
        self.codes = df['code'].fillna("").astype(str).tolist() # ì»¬ëŸ¼ëª… í™•ì¸ í•„ìš”
        self.tokenizer = tokenizer
        self.max_len = max_len
    
    def __len__(self):
        return len(self.codes)
    
    def __getitem__(self, idx):
        enc = self.tokenizer(self.codes[idx], truncation=True, padding='max_length', 
                             max_length=self.max_len, return_tensors='pt')
        return {'input_ids': enc['input_ids'].squeeze(0), 
                'attention_mask': enc['attention_mask'].squeeze(0)}

# ================= ì¶”ì¶œ í•¨ìˆ˜ =================
def extract_probs(df, model, tokenizer, filename):
    dataset = SimpleCodeDataset(df, tokenizer)
    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
    
    all_probs = []
    model.eval()
    
    print(f"ğŸ”„ Extracting from {filename}...")
    with torch.no_grad():
        for batch in tqdm(loader):
            input_ids = batch['input_ids'].to(device)
            mask = batch['attention_mask'].to(device)
            
            logits = model(input_ids, mask)
            probs = torch.softmax(logits, dim=1) # Logits -> Probabilities ë³€í™˜
            all_probs.append(probs.cpu().numpy())
            
    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì €ì¥
    import numpy as np
    all_probs = np.concatenate(all_probs, axis=0)
    cols = [f"unix_prob_{i}" for i in range(NUM_CLASSES)]
    prob_df = pd.DataFrame(all_probs, columns=cols)
    
    # ì›ë³¸ ë¼ë²¨ì´ ìˆë‹¤ë©´ ë¶™ì—¬ì£¼ê¸° (í•™ìŠµìš©)
    if 'label' in df.columns:
        prob_df['label'] = df['label'].values
        
    save_path = os.path.join(SAVE_DIR, f"unix_{filename}")
    prob_df.to_parquet(save_path)
    print(f"âœ… Saved to {save_path}")

# [01_extract_unixcoder.py ì˜ ë§¨ ë§ˆì§€ë§‰ ë¶€ë¶„ ìˆ˜ì •]

# ... (ìœ„ìª½ í•¨ìˆ˜ ì •ì˜ ë“±ì€ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”) ...

# ================= ì‹¤í–‰ =================
# 1. ëª¨ë¸ ë¡œë“œ
print("Loading Model...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
model = UniXcoderClassifier(BASE_MODEL, NUM_CLASSES).to(device)
model.load_state_dict(torch.load(MODEL_PATH, map_location=device))

# 2. ë°ì´í„° ë¡œë“œ
# (Train, Valì€ ì´ë¯¸ features í´ë”ì— ìˆìœ¼ë‹ˆ ì£¼ì„ ì²˜ë¦¬í•´ì„œ ê±´ë„ˆëœë‹ˆë‹¤!)
# train_df = pd.read_parquet(os.path.join(DATA_DIR, "task_b_training_set.parquet"))
# val_df = pd.read_parquet(os.path.join(DATA_DIR, "task_b_validation_set.parquet"))

# extract_probs(train_df, model, tokenizer, "train.parquet")
# extract_probs(val_df, model, tokenizer, "val.parquet")

# [ì¤‘ìš”] ëˆ„ë½ë˜ì—ˆë˜ Test ì…‹ë§Œ ì¶”ì¶œ ì‹¤í–‰!
print("ğŸš€ Processing Test Set...")
test_path = os.path.join(DATA_DIR, "test.parquet")

if os.path.exists(test_path):
    test_df = pd.read_parquet(test_path)
    extract_probs(test_df, model, tokenizer, "test.parquet") # -> features/unix_test.parquet ìƒì„±ë¨
else:
    print(f"âŒ '{test_path}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")