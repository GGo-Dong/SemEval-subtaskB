import os
import torch
import pandas as pd
import numpy as np
from torch.utils.data import Dataset
from sklearn.metrics import f1_score, accuracy_score
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    BitsAndBytesConfig,
    TrainingArguments, 
    Trainer,
    DataCollatorWithPadding
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType

# ================= ì„¤ì • =================
MODEL_ID = "Qwen/Qwen2.5-Coder-7B-Instruct" 
DATA_DIR = "data"
OUTPUT_DIR = "qwen_qlora_checkpoints"

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
MAX_LEN = 512
BATCH_SIZE = 4
GRAD_ACCUM = 4
EPOCHS = 1        
LR = 2e-4

# ì´ì–´í•˜ê¸°ë¥¼ ìœ„í•œ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
CHECKPOINT_PATH = os.path.join(OUTPUT_DIR, "checkpoint-1000")

# ================= ë°ì´í„° ë¡œë“œ ë° ì •ì œ =================
print("ğŸ“‚ Loading Data...")
full_train_df = pd.read_parquet(os.path.join(DATA_DIR, "task_b_training_set.parquet"))

# ì»¬ëŸ¼ëª… í†µì¼
if 'code' not in full_train_df.columns: full_train_df['code'] = full_train_df['text']

# [ì •ì œ] 125ê¸€ì ë¯¸ë§Œ ì œê±° (ë…¸ì´ì¦ˆ ì œê±°)
print("ğŸ§¹ Cleaning Data: Removing codes shorter than 125 chars...")
clean_mask = (full_train_df['code'].fillna("").str.strip().str.len() >= 125)
train_df_clean = full_train_df[clean_mask]

# [í•™ìŠµ ë°ì´í„° ìƒ˜í”Œë§] 10ë§Œ ê°œ
if len(train_df_clean) > 100000:
    train_df = train_df_clean.sample(n=100000, random_state=42).reset_index(drop=True)
else:
    train_df = train_df_clean.reset_index(drop=True)

print(f"ğŸ“‰ Final Training Data Size: {len(train_df)} samples")

# [ê²€ì¦ ë°ì´í„° ë¡œë“œ ë° ì¶•ì†Œ] - ì—¬ê¸°ê°€ ìˆ˜ì •ëœ í•µì‹¬ ë¶€ë¶„!
print("ğŸ“‰ Loading Validation Data...")
full_val_df = pd.read_parquet(os.path.join(DATA_DIR, "task_b_validation_set.parquet"))

# [ì†ë„ í–¥ìƒ] ê²€ì¦ ë°ì´í„° 2ë§Œ ê°œë¡œ ìƒ˜í”Œë§ (2ì‹œê°„ -> 15ë¶„ ë‹¨ì¶•)
val_df = full_val_df.sample(n=20000, random_state=42).reset_index(drop=True)

if 'code' not in val_df.columns: val_df['code'] = val_df['text']
print(f"âœ… Validation Data Downsampled: {len(full_val_df)} -> {len(val_df)} (Speed Up!)")

# ================= í† í¬ë‚˜ì´ì € & ë°ì´í„°ì…‹ =================
print("âš™ï¸ Loading Tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# HuggingFace Dataset ë³€í™˜
from datasets import Dataset as HFDataset

def preprocess_function(examples):
    return tokenizer(examples['text'], truncation=True, max_length=MAX_LEN)

train_ds = HFDataset.from_pandas(train_df[['code', 'label']].rename(columns={'code': 'text'}))
val_ds = HFDataset.from_pandas(val_df[['code', 'label']].rename(columns={'code': 'text'}))

train_tokenized = train_ds.map(preprocess_function, batched=True)
val_tokenized = val_ds.map(preprocess_function, batched=True)

# ================= ëª¨ë¸ ì¤€ë¹„ (QLoRA) =================
print("ğŸ¤– Loading Model with 4-bit Quantization...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_ID,
    num_labels=11,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

model.config.pad_token_id = tokenizer.pad_token_id
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)

peft_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    inference_mode=False,
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    target_modules=["q_proj", "v_proj"]
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# ================= í•™ìŠµ ì„¤ì • =================
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, predictions),
        "f1": f1_score(labels, predictions, average="macro")
    }

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    learning_rate=LR,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRAD_ACCUM,
    
    num_train_epochs=EPOCHS,     
    
    weight_decay=0.01,
    eval_strategy="steps",       
    
    eval_steps=1000,             
    save_strategy="steps",
    save_steps=1000,
    save_total_limit=2,
    
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    fp16=True,
    logging_steps=50,
    report_to="none",
    dataloader_num_workers=4
)

# ================= íŠ¸ë ˆì´ë„ˆ ì‹¤í–‰ (Resume Logic í¬í•¨) =================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=val_tokenized,
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
)

# ì²´í¬í¬ì¸íŠ¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ ì´ì–´í•˜ê¸°
if os.path.exists(CHECKPOINT_PATH):
    print(f"ğŸš€ Resuming Training from {CHECKPOINT_PATH}...")
    trainer.train(resume_from_checkpoint=CHECKPOINT_PATH)
else:
    print("ğŸš€ Start Training Qwen-7B (QLoRA) from Scratch...")
    trainer.train()

print("ğŸ’¾ Saving Final LoRA Adapter...")
trainer.save_model("models/qwen_qlora_final")