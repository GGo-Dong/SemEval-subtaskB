import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig
from peft import PeftModel
from tqdm import tqdm
import os
import numpy as np

# ================= ì„¤ì • =================
BASE_MODEL_ID = "Qwen/Qwen2.5-Coder-7B-Instruct"
ADAPTER_PATH = "models/qwen_qlora_final"
DATA_DIR = "data"
SAVE_DIR = "features"   # ì—¬ê¸°ì— ì €ì¥ë©ë‹ˆë‹¤
BATCH_SIZE = 8
MAX_LEN = 512
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ================= ë°ì´í„°ì…‹ =================
class CodeDataset(Dataset):
    def __init__(self, df, tokenizer, max_len=512):
        self.texts = df['code'].fillna("").astype(str).tolist()
        self.tokenizer = tokenizer
        self.max_len = max_len
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        enc = self.tokenizer(self.texts[idx], truncation=True, padding='max_length', 
                             max_length=self.max_len, return_tensors='pt')
        return {
            'input_ids': enc['input_ids'].squeeze(0),
            'attention_mask': enc['attention_mask'].squeeze(0)
        }

# ================= ë©”ì¸ ì‹¤í–‰ =================
print("ğŸ“‚ Loading Data...")
test_df = pd.read_parquet(os.path.join(DATA_DIR, "test.parquet"))
if 'code' not in test_df.columns: test_df['code'] = test_df['text']

print("ğŸ¤– Loading Qwen Model & Adapter...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)
# [ì¤‘ìš”] ì•™ìƒë¸”ìš© í™•ë¥  ì¶”ì¶œ ì‹œì—ëŠ” íŒ¨ë”© ì„¤ì • ìœ ì§€
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

base_model = AutoModelForSequenceClassification.from_pretrained(
    BASE_MODEL_ID,
    num_labels=11,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)
base_model.config.pad_token_id = tokenizer.pad_token_id

# í•™ìŠµëœ LoRA ì–´ëŒ‘í„° ì¥ì°©
model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
model.eval()

# ================= í™•ë¥  ì¶”ì¶œ ë° ì €ì¥ =================
print("ğŸš€ Extracting Probabilities for Ensemble...")
test_loader = DataLoader(CodeDataset(test_df, tokenizer, MAX_LEN), batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

all_probs = []

with torch.no_grad():
    for batch in tqdm(test_loader):
        input_ids = batch['input_ids'].to(device)
        mask = batch['attention_mask'].to(device)
        
        logits = model(input_ids, attention_mask=mask).logits
        probs = torch.softmax(logits, dim=1) # í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜
        all_probs.extend(probs.cpu().numpy().astype(np.float16)) # ìš©ëŸ‰ ì ˆì•½

# ì €ì¥
if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)

prob_cols = [f'qwen_prob_{i}' for i in range(len(all_probs[0]))]
out_df = pd.DataFrame(all_probs, columns=prob_cols)

# ë‚˜ì¤‘ì— ìˆœì„œ ì„ì„ ë°©ì§€ë¥¼ ìœ„í•´ IDë„ í•¨ê»˜ ì €ì¥
id_col = 'id' if 'id' in test_df.columns else test_df.columns[0]
out_df['id'] = test_df[id_col].values

save_path = os.path.join(SAVE_DIR, "qwen_test.parquet")
out_df.to_parquet(save_path)

print(f"âœ… Saved Features to: {save_path}")
print("ğŸ‘‰ Now you can run '10_final_ensemble.py'!")